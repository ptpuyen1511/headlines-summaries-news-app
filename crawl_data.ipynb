{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import urllib\n",
    "import urllib.error\n",
    "import urllib.request\n",
    "from urllib.parse import urlparse, quote\n",
    "import posixpath\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter URL (from mutcrawler)\n",
    "class FilterURL(object):\n",
    "    def __init__(self, urls=[], max_length = 128, confine=None, exclude=[]):\n",
    "        self.urls = urls\n",
    "\n",
    "        self.max_len = max_length           # Max length of link\n",
    "        self.confine_prefix = confine       # Limit search to this prefix\n",
    "        self.exclude_prefixes = exclude     # URL prefixes NOT to visit\n",
    "\n",
    "        # To remove iamge, video, rss link\n",
    "        self.ignore_exts = { '.png', '.jpg', '.jpeg', '.gif', '.mp3', '.mp4', '.rss', '.pdf', '.css', '.js', '.zip', '.gz'}\n",
    "\n",
    "        # To check if a string is url\n",
    "        self.regex = re.compile(\n",
    "                        r'^(?:http|ftp)s?://' # http:// or https://\n",
    "                        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|' #domain...\n",
    "                        r'localhost|' #localhost...\n",
    "                        r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})' # ...or ip\n",
    "                        r'(?::\\d+)?' # optional port\n",
    "                        r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "    # Check if a string is url\n",
    "    def is_url(self, url):\n",
    "        return re.match(self.regex, url) is not None\n",
    "\n",
    "\n",
    "    # Check if a url is image, video, rss, pdf\n",
    "    def is_in_ignore_exts(self, url):\n",
    "        if posixpath.splitext(urlparse(url).path)[1] in self.ignore_exts:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "\n",
    "    # Reduce URLs into some canonical form before visiting\n",
    "    # http://abc.com/test.html\\#hehe --> http://abc.com/test.html\n",
    "    def url_condense(self, url):\n",
    "        if url is None:\n",
    "            return None\n",
    "\n",
    "        base, frag = urllib.parse.urldefrag(url)\n",
    "        return base\n",
    "\n",
    "\n",
    "    # Remove all queries in url\n",
    "    def clean_url_query(self, url):\n",
    "        if url is None:\n",
    "            return None\n",
    "\n",
    "        obj = urlparse(url)\n",
    "        return '%s://%s%s' % (obj.scheme, obj.netloc, obj.path)   \n",
    "\n",
    "\n",
    "    # Normaliza url (if url has unicode)\n",
    "    def url_encode(self, url):\n",
    "        if url is None:\n",
    "            return None\n",
    "\n",
    "        return quote(url, safe=\"%/:=&?~#+!$,;'@()*[]\") \n",
    "    \n",
    "    # Pass if the URL has len <= 128\n",
    "    def len_too_long(self, url):\n",
    "        return len(url) > self.max_len\n",
    "\n",
    "\n",
    "    # Pass if the URL has the correct prefix, or none is specified\n",
    "    def prefix_ok(self, url):\n",
    "        return (self.confine_prefix is None or\n",
    "                    url.startswith(self.confine_prefix))\n",
    "\n",
    "\n",
    "    # Pass if the URL does not match any exclude patterns\n",
    "    def exclude_ok(self, url):\n",
    "        prefixes_ok = [not url.startswith(p) for  p in self.exclude_prefixes]\n",
    "\n",
    "        return all(prefixes_ok)\n",
    "\n",
    "\n",
    "    # Get urls is ok\n",
    "    def get_okurls(self):\n",
    "        urls_ok = set()\n",
    "\n",
    "        # Traverse each url to normalize and check\n",
    "        for url in self.urls:\n",
    "            # Is urls\n",
    "            if self.is_url(url) is False:\n",
    "                continue\n",
    "\n",
    "            # Is image, video, rss\n",
    "            if self.is_in_ignore_exts(url) is True:\n",
    "                continue\n",
    "\n",
    "            # Check len\n",
    "            if self.len_too_long(url):\n",
    "                continue\n",
    "\n",
    "            # Check prefix\n",
    "            if self.prefix_ok(url) is False:\n",
    "                continue\n",
    "\n",
    "            if self.exclude_ok(url) is False:\n",
    "                continue\n",
    "\n",
    "            # Clean url\n",
    "            tmp = self.clean_url_query(url)\n",
    "            tmp = self.url_condense(tmp)\n",
    "            tmp = self.url_encode(tmp)\n",
    "\n",
    "            # Everything ok --> add to urls_ok\n",
    "            urls_ok.add(tmp)\n",
    "\n",
    "        return list(urls_ok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_links(urls=['https://e.vnexpress.net/news/news', 'https://e.vnexpress.net/news/business', 'https://e.vnexpress.net/news/travel', 'https://e.vnexpress.net/news/life', 'https://e.vnexpress.net/news/sports', 'https://e.vnexpress.net/news/world']):\n",
    "    '''\n",
    "    Get all links from seek urls\n",
    "    \n",
    "    Args:\n",
    "        urls (str): The seek urls\n",
    "\n",
    "    Returns:\n",
    "        links (list): A list of all links\n",
    "    '''\n",
    "\n",
    "    links = []\n",
    "\n",
    "    for seek in urls:\n",
    "        # Send a GET request to the URL and get the response\n",
    "        response = requests.get(seek)\n",
    "\n",
    "        # Parse the HTML content of the response using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Get links from page\n",
    "        tags = soup('a')\n",
    "        for tag in tags:\n",
    "            url = tag.get('href')\n",
    "            if url is not None and url.startswith(seek):\n",
    "                if url not in links:\n",
    "                    links.append(url)\n",
    "        \n",
    "\n",
    "    return FilterURL(links).get_okurls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(url):\n",
    "    '''\n",
    "    Get the content of the url\n",
    "    The response from the URL is parsed into a dictionary containing the extracted information\n",
    "\n",
    "    Args:\n",
    "        url (str): The url to get the content from\n",
    "\n",
    "    Returns:\n",
    "        info (dict): A dictionary containing: title, date, text, author, and url\n",
    "    '''\n",
    "\n",
    "    # Send a GET request to the URL and get the response\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the response using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    info = {\n",
    "        'url': url,\n",
    "        'is_valid': False\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Find the title of the page\n",
    "        title = soup.find('h1', {'class': 'title_post'}).text\n",
    "\n",
    "        # Find the date\n",
    "        author_and_date = soup.find('div', {'class': 'author'}).text\n",
    "        author = author_and_date.split('&nbsp')[0].replace('By ', '')\n",
    "        date = author_and_date.split('&nbsp')[1].split('|')[0]\n",
    "\n",
    "        # Find the text\n",
    "        description = soup.find('span', {'class': 'lead_post_detail row'}).text\n",
    "        paras = soup.find_all('p', {'class': 'Normal'})\n",
    "        text = description.strip() + '\\n' + '\\n'.join([p.text.strip() for p in paras])\n",
    "\n",
    "        # Create a dictionary to store the extracted information\n",
    "        info = {\n",
    "            'title': title.strip(),\n",
    "            'date': date.strip(),\n",
    "            'text': text.strip(),\n",
    "            'author': author.strip(),\n",
    "            'category': url.split('/')[4],\n",
    "            'url': url,\n",
    "            'is_valid': True\n",
    "        }\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Indonesian volcano eruption forces evacuations, airport closure',\n",
       " 'date': 'April 18, 2024',\n",
       " 'text': 'Indonesia shut a provincial airport and evacuated hundreds of people from the vicinity of the Ruang volcano after it belched explosive plumes of lava, rocks and ash for days, officials said on Thursday, declaring the highest alert on the situation.\\nWednesday’s dramatic eruption of the volcano on a remote island in the province of North Sulawesi threw a fiery-red column of lava, incandescent rock and ash as much as three km (two miles) into the sky.\\nPurple flashes of lightning rent the sky above the erupting volcano, videos on social media showed.\\n\"We’re running, guys,\" said one witness who filmed the eruption while scrambling to evacuate. \"We are escaping because the ash is coming close.\"\\nMore than 800 people were evacuated from the area, with authorities widening the evacuation zone further after the volcanology agency raised the alert status.\\n\"The potential for further eruption is still high, so we need to remain alert,\" agency official Heruningtyas Desi Purnamasari told reporters on Thursday, blaming a rapid escalation in volcanic activity.\\nThe agency had also received reports that falling rocks and ash damaged homes and forced a nearby hospital to evacuate, the official said.\\nTransport authorities shut the airport in the provincial capital of Manado to protect against the showers of ash from the eruption.\\nBudget airline Air Asia canceled flights with nine airports in East Malaysia and Brunei after aviation authorities warned of a safety threat.\\nOfficials have cordoned off an area of 6 km around the volcano and are evacuating more residents, some from the neighboring island of Tagulandang, said Abdul Muhari, spokesperson of the disaster mitigation agency.\\nAbout 1,500 of those in high-risk areas needed to be immediately evacuated, he added, while almost 12,000 more stand to be affected.\\nOfficials have also flagged the risk of a tsunami if parts of the mountain collapse into the ocean below. About 400 people were killed in a tsunami unleashed by a previous eruption of the volcano in 1871.',\n",
       " 'author': 'Reuters',\n",
       " 'category': 'world',\n",
       " 'url': 'https://e.vnexpress.net/news/world/indonesian-volcano-eruption-forces-evacuations-airport-closure-4735823.html',\n",
       " 'is_valid': True}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_content('https://e.vnexpress.net/news/world/indonesian-volcano-eruption-forces-evacuations-airport-closure-4735823.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
